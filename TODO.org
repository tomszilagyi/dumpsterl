Dataspec rewrite

The purpose of this rewrite: create a durable, extendable and
user-friendly framework for data exploration in NoSQL environments.

** generalized and pluggable type system
- count parent categories and imbedded categories, too
- count skipped rows, too (where accessor throws)
- allow dynamic subtypes to have further subtypes
  ie. allow types() to contain something like:
    { {tuple, '_'}, ... } %% subtypes for any (concrete) size tuple

** standard and pluggable spec format
- also for programmatic report creation

** advanced features for data mining
- sampling
- tag each data with arbitrary attributes
  - timestamp (when was this data created)
  - key (to read back the whole record)

** ideas for development
- add complex types to the type hierarchy, eg.
  - {list_of, T} dynamic type where a list has elements of the same type
  - commonly used complex types e.g. proplist
- cut intermediary stages of the hierarchy, e.g. if they are all
  integers, get rid of 'T' and 'numeric'. Formally, all nodes with
  only one child should be removed and replaced by the child.
- 'decision tree compiler' for quick computation of values' subtypes
- more efficient in-memory representation of tree, for faster updates
  (store all nodes in a flat ets table keyed by class, since they are
  unique; updating a node does not involve rebuilding the whole data
  structure)
- account each data term only in leaf node, and when assembling the
  tree, derive parent nodes based on the union of children...
- parallelize: send data to N trees in round-robin fashion, merge
  results regularly for each chunk or at the end
  (N > number of processors)
- read schema.DAT to learn about record fields
- treat dict types similar to tuples/records, where the keys
  correspond to field names (with the notable exception that not all
  instances necessarily have the same sets of keys, maybe only a
  subset). This is also how maps should be implemented.
  The GUI could show a toplevel stat of what keys there are (with what
  freq, etc) and the below level individual per-key stats for values.
- don't add the same value multiple times to samples, to maximize the
  number of unique values shown. Instead, for each collected value,
  maintain per-value stats to show how it is distributed across the
  population.

** GUI browser concept
- two panes: left for navigation, right for details
- navigation pane:
  - type hierarchy stack
    see where the view is located; clicking on any parent
    jumps up to that level
    eg. T -> tuple -> {record, {kcase, 21}}
  - subtype / element list
    shows the list of subspecs below the currently selected one;
    clicking on any of them navigates the view down to that level

    in case of complex structures, this shows a list of these specs,
    allowing the user to click on one of them. The view is then
    directed to the toplevel spec of that element, as a further level
    down the hierarchy.
    eg. T -> tuple -> {record, {kcase, 21}} -> element(2) or #kcase.cid

    the transitions may be distinguished by coloring the entry
    in the type hierarchy stack, e.g.
      subtype: light grey;
      field / element / item: light orange

- details pane:
  - statistics visualization panel
  - private data visualization panel

- statistics and private data should expose accessors so the
  gui can be displayed on a modular basis (i.e. knowledge of the
  data structure, and how to display it, should reside in the
  statistics or private data module; gui code should remain generic).

- the field attributes information for records to be stored in Ext
  - gather field attributes automatically, when first encountering
    a record type
  - the means depends on the data source:
    - default: record attribute definitions in loaded beam/source
      files, read once at the beginning, see rec_attrs in ds_opts.
    - mnesia -> mnesia:table_info(T, attributes)
        This does not make sense: if dataspec is run on a system where
        mnesia is up and running, the code accessing the tables is
        also loaded, so we can get the same info (and more) from the
        beam/source attributes.
    - disk_log -> read schema.DAT
        Might be useful if a bare Erlang node is used to run the
        dataspec probe.

- when showing a type with sub-fields, the columns in the lower tab
  should be Field, Attribute, Type instead of Type, Count (since these
  counts are always all identical to that of the complex type).
  Field is the field number and Attribute is the field name.

*** additional gui elements
- search: allow entering a piece of string, data, etc.
- toolbar: load, save, generate report, exit, etc.
- manually add field information in case it could not be
  gathered automatically
- manually supply timestamp format function
  e.g. if it turns out the ts is in gregorian seconds, allow
  user to set fun calendar:gregorian_seconds_to_datetime/1.

** test commands

%% whole kcase table with annotations from disk log:
ds_drv:spec_disk_log("/kred/live/system/db/kcase.DCD", {0, [{ts, #kcase.create_date}, {key, #kcase.cid}]}, inf, [{progress, 10000}, {dump, "/tmp/ds.bin"}]).

